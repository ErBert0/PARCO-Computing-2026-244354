# Parallel Computing Project: MPI SpMV Algorithm

## 1. Algorithm Overview

This project focuses on the optimization of **Sparse Matrix-Vector Multiplication (SpMV)** using the `Message Passing Interface (MPI)` standard on a `distributed memory system`.

### 1.1 Data Representation and Algorithm Parallelization

- **Data Partitioning and Distribution**: The input matrix is read (or generated) by the root process and sorted/distributed following a `1D Row-Cyclic partitioning`, such that each row $i$ is assigned to process $P_k$ where $k = i \pmod{size}$.
- **Local Storage**: Each MPI process converts its assigned non-zero elements into the `Compressed Sparse Row (CSR)` format.
- **Communication**: Each rank generates a portion of the X vector. Before the start of the computation, an MPI_AllgatherV operation gathers the full vector X on all processes.
This allows each process to compute it's chunk of the output vector y independently.


### 1.2 Scaling Modes
The application supports:

- **Strong Scaling**: Fixed matrix size (input matrix) with increasing number of processes (up to 128).

- **Weak Scaling**: The matrix size increases with the number of processes. The matrix used are synthetic (random) with a fixed number of rows and non-zeros per processor.

### 1.3 Benchmark Dataset

The performance testing is conducted on a set of 5 matrices from the [SuiteSparse Matrix Collection](https://sparse.tamu.edu/), selected to represent different sparsity patterns 

| Matrix Name | Rows | Non-Zeros (NNZ) | Type | Link |
| :--- | :--- | :--- | :--- | :--- |
| **ASIC_320k** | ~321k | ~1,9 M | Real | [Link](https://sparse.tamu.edu/Sandia/ASIC_320k) |
| **kkt_power** |  ~2.0 M | ~12.7 M | Real | [Link](https://sparse.tamu.edu/KKT/kkt_power) |
| **mawi** |  ~128 M | ~ 270 M | Real | [Link](https://sparse.tamu.edu/MAWI/mawi_201512020130) |
| **nlpkkt80** | ~1 M | ~28 M | Real | [Link](https://sparse.tamu.edu/Schenk/nlpkkt80) |
| **ecology2** | ~1 M | ~5 M | Real | [Link](https://sparse.tamu.edu/McRae/ecology2) |

## 2. Structure

- **/src**: Contains the C++ source code and the Matrix Market I/O librarys.
- **/scripts**: Contains the `.pbs` script for submitting the benchamrks to the cluster.
- **/matrices**: Stores the input matrices in .mtx format.
- **/results**: Contains the final excel file (`.xlsx`) with all the data collected and the `.csv` output files generated by the jobs.
- **/plots**: Contains the final graphs and figures used in the report.

## 3. Compiler Version and Environment

All benchmarks were conducted on the university cluster, using:

- **Compiler:** `mpic++` 
- **Modules:** 
    - `gcc91`
    - `mpich-3.2.1--gcc-9.1.0`
- **Compilation Flags:** `mpic++ -O3 -march=native -o <output_name> src/main.cpp src/mmio.c`

## 4. How to Compile and Run

### Step 0: Clone the Repository
```bash
git clone https://github.com/ErBert0/PARCO-Computing-2026-244354.git
cd PARCO-Computing-2026-244354/Deliverable_2
```

### Step 1: Configuration
The main automation is handled by the PBS script, located in the `scripts/` folder.

Before submitting the job, check the configuration section at the top of the file, where you will be able to modify:
- `BENCHMARK_MODE`: Set to "strong" or "weak".
- `PROCS_TO_TEST`: Define the list of MPI processes (e.g., 1 2 4 8 16 32).
- `MATRICES`: Make sure the names of the matrices correspond to the matrix in your `matrices/` folder


### Step 2: Running the Benchmark

To run the entire benchmark (with strong or weak scaling) simply submit the job to the cluster:

```bash
cd PARCO-Computing-2026-244354/Deliverable_2
qsub scripts/run_benchmark.pbs
```

The PBS will:
- Load of the necessary modules
- Compile the source code
- Run the loop over the selected matrices and processor counts
- Generate a `CSV` file in the `results/` folder



### Step 3 (Optional): Manual Compilation and Runs

If you wish to manually compile and test the code on the cluster (or locally) make sure to follow these steps:

```bash
# Load modules (If you are on the cluster)
module load gcc91
module load mpich-3.2.1--gcc-9.1.0

# Compile
mpic++ -O3 -march=native -o spmv_executable.o src/main.cpp src/mmio.c

# Run (Strong Scaling)
mpirun -np 4 ./spmv_executable.o matrices/1_ASIC.mtx

# Run (Weak Scaling)
mpirun -np 4 ./spmv_executable.o --weak 10000 50
```


## 5. Input and Output

- **Input:** The C++ program accepts different arguments depending on the desired scaling test to execute.
    -  `Strong Scaling`: Accepts the path to a Matrix Market File `(.mtx)`
    -  `Weak Scaling`: Accepts the flags `--weak <rows_per_proc> <nnz_per_row>`
- **Output:** 
    - `STD Output`: Debug Informations and Detailed Timings, plus the CSV string
    - `CSV file`: The PBS script parses the output and saves a structured CSV in `results/`.

The CSV format is: `mode, matrix, procs, run_id, total_time, comm_time, comp_time, gflops`

